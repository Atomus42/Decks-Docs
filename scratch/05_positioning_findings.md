# Agent 5 â€” Evidence & Website Miner: Positioning Findings

## Purpose

This document audits every public-facing claim made by ArcaScience on its websites (arcascience.ai, arcascienceval.live) and in its sales deck (ArcaScience Deck 2026), grades each for regulatory risk, and provides safer replacements calibrated to MHRA expectations. It also establishes a binding "claims we will NOT make" list and messaging principles for all future regulator-facing communications.

The central insight driving this document: MHRA Deputy Director Allison stated on record that the "in seconds" claim sent her "antibodies through the roof" and raised "loads and loads of concerns." She also said, "You make it sound so easy, and it's so so hard." Every claim below is evaluated through that lens.

---

## 1. Claims Inventory

### Risk Rating Key

| Rating | Meaning |
|--------|---------|
| **HIGH** | Claim would likely trigger active skepticism, damage credibility, or raise governance concerns if seen by MHRA assessors. Must be removed or completely reframed before any regulator-facing communication. |
| **MEDIUM** | Claim is defensible but uses framing that regulators find uncomfortable (absolutes, speed claims, implied replacement of judgment). Should be softened. |
| **LOW** | Claim is acceptable but could benefit from more precise language or context. |

---

### 1.1 Speed and Efficiency Claims

| # | Claim | Source | Risk Level | Why It Is Risky for Regulators |
|---|-------|--------|------------|-------------------------------|
| 1 | "What used to take 18 months now completes in seconds" | arcascience.ai | **HIGH** | Directly triggered Allison's strongest negative reaction. Implies the system performs the entire BRA in seconds, which conflates data consolidation with expert judgment. Regulators hear this as "we skip the hard part." |
| 2 | "Build your drug's BRE in seconds / Don't spend months looking for it" | arcascience.ai, Deck 2026 | **HIGH** | Same issue. The phrase "Build your BRE" implies a finished evaluation, not a data assembly step. "Don't spend months looking for it" trivializes the rigor regulators invest. Allison quoted this claim verbatim as problematic. |
| 3 | "18 months of work done in mere seconds" | Deck 2026 | **HIGH** | Hyperbolically compresses 18 months to seconds. Even if referring only to data retrieval, no regulator will interpret it that way. Suggests automated judgment, which is anathema to an assessor's professional identity. |
| 4 | "60% faster evaluation time" | arcascience.ai | **MEDIUM** | "Evaluation" is a loaded word for regulators -- it implies the system evaluates, which is the assessor's role. The metric itself lacks context: 60% faster than what baseline? Measured how? |
| 5 | Sanofi "60% reduction in PSUR cycle time" | arcascienceval.live | **MEDIUM** | More defensible because it is attributed to a named client and scoped to PSUR cycle time (a process metric, not a judgment metric). But still lacks published methodology or independent verification. |
| 6 | "Reduce BRA Project Time by 80%" | Deck 2026 | **HIGH** | 80% reduction in "BRA project time" is both extreme and vague. Regulators will ask: 80% of what? The data gathering? The analysis? The committee deliberation? If the answer is "all of it," credibility collapses. |

### 1.2 Volume and Scale Claims

| # | Claim | Source | Risk Level | Why It Is Risky for Regulators |
|---|-------|--------|------------|-------------------------------|
| 7 | "24 proprietary AI models process millions of data points" | arcascience.ai | **MEDIUM** | "Proprietary" without transparency is a red flag for regulators who need to audit methodology. "Millions of data points" is vague. More is not automatically better in regulatory science -- quality and relevance matter more than volume. |
| 8 | "100+ billion data points" / "100B+ data points in profiling database" | arcascience.ai, arcascienceval.live | **MEDIUM** | Impressive-sounding but raises the question Allison explicitly asked: how do you ensure the quality of those data sources? Volume claims without quality claims are counterproductive with regulators. Allison's concern about noisy, unstructured post-marketing data directly challenges this framing. |
| 9 | "World's largest benefit-risk intelligence database" | arcascience.ai | **MEDIUM** | Superlative claim that is unverifiable and invites challenge. Regulators will ask: largest by what measure? Independently audited? What does "largest" add if the data is not validated for regulatory use? |
| 10 | "9x more insights detected" | arcascience.ai | **HIGH** | "Insights" is subjective. 9x more than what? Than a human assessor? That implies the system outperforms human judgment, which regulators will find both arrogant and unverifiable. The word "detected" implies the system makes clinical determinations. |
| 11 | "3x DDI detection boost" | arcascience.ai | **MEDIUM** | Drug-drug interaction detection is a specific, measurable claim. But "3x boost" over what baseline? Over manual review? Over competitor tools? Without published methodology, this is a marketing statistic, not a scientific claim. |

### 1.3 Regulatory Acceptance and Track Record Claims

| # | Claim | Source | Risk Level | Why It Is Risky for Regulators |
|---|-------|--------|------------|-------------------------------|
| 12 | "100% regulatory acceptance rate with FDA, EMA, PMDA" | arcascience.ai, arcascienceval.live | **HIGH** | This is the single most dangerous claim for MHRA engagement. It implies that regulatory agencies have endorsed or validated the ArcaScience platform. In reality, agencies accept submissions from sponsors -- they do not certify the tools sponsors used. MHRA will immediately recognize this as a misattribution of agency authority. It also creates the impression that using ArcaScience guarantees approval, which is false and could be seen as misleading. |
| 13 | "Supporting 50+ regulatory submissions across 12 therapeutic areas" | arcascience.ai | **LOW** | This is factual and defensible if accurate. However, "supporting" is vague -- does it mean the platform was used for data gathering, or that ArcaScience consultants contributed to the submission? Needs clarification. |
| 14 | "50+ regulatory submissions accepted" | arcascienceval.live | **HIGH** | Shifts from "supporting" to "accepted," which implies that the submissions were approved because of ArcaScience. Regulatory acceptance of a submission depends on the totality of the sponsor's data and arguments, not on the tool used to assemble them. |
| 15 | "ArcaScience won 100% of its deals when facing traditional consulting companies" | arcascience.ai | **LOW** | Not directly risky for regulators, but suggests a sales-focused rather than science-focused culture. Irrelevant and potentially off-putting in any regulator-facing context. |
| 16 | "Currently under review by the MHRA" | Deck 2026 | **HIGH** | Extremely dangerous. MHRA is not "reviewing" ArcaScience as a product for endorsement. The current engagement is an exploratory conversation. If MHRA sees this claim, it will destroy trust immediately. Allison's team would likely terminate the engagement if they believed ArcaScience was claiming MHRA review/endorsement. This must be removed from the deck before any further contact. |

### 1.4 Capability and Scope Claims

| # | Claim | Source | Risk Level | Why It Is Risky for Regulators |
|---|-------|--------|------------|-------------------------------|
| 17 | "AI-Driven Benefit-Risk Analysis Across the Full Drug Lifecycle" | arcascienceval.live | **HIGH** | "AI-Driven Benefit-Risk Analysis" implies the AI performs the analysis. For regulators, benefit-risk analysis is a human judgment activity. "Full Drug Lifecycle" is an overstatement -- the system does not handle omics data, imaging, or (per the transcript) the deep contextual judgment that post-authorization work requires. Allison explicitly challenged this claim. |
| 18 | Generates "PSUR/PBRER, Risk Management Plans, CTD 2.5 sections, HEOR reports" | arcascience.ai | **MEDIUM** | "Generates" implies finished regulatory documents produced by AI. In reality, the system pre-populates templates that require expert review and completion. Regulators who sign off on these documents will not accept that they were "generated" by a machine. |
| 19 | "47 countries harmonized" | arcascienceval.live | **LOW** | Vague but not offensive. Needs clarification: harmonized in what sense? Data coverage? Regulatory mapping? Template availability? |
| 20 | "By 2025, 80% of pharma will have adopted AI benefit-risk-enabled solutions" (IDC quote) | arcascience.ai | **LOW** | Third-party attribution makes this lower risk, but the prediction has not materialized (it is now 2026). Using an expired prediction undermines credibility. Should be removed or updated. |

---

## 2. "Claims We Will NOT Make" List

The following statements must never appear in any communication with MHRA -- written, verbal, slide deck, email, or demo walkthrough. This list is binding on all ArcaScience personnel involved in the MHRA engagement.

### Absolute Prohibitions

1. **We will NOT claim that ArcaScience "does," "performs," "conducts," or "completes" benefit-risk assessment.** The system supports, accelerates, and informs the assessment. The assessment itself is performed by qualified human experts.

2. **We will NOT claim that the system produces results "in seconds."** We will describe what happens in seconds (data retrieval, cross-referencing, initial structuring) and what requires hours, days, or weeks (expert review, contextual judgment, committee deliberation).

3. **We will NOT claim "100% regulatory acceptance rate" or any formulation that implies regulatory agencies have endorsed, certified, or validated the ArcaScience platform.** We will say: "Our outputs have been incorporated into N regulatory submissions by our pharmaceutical clients."

4. **We will NOT claim that the system "replaces," "substitutes for," or "automates" assessor judgment.** We will always frame the system as augmenting, supporting, and informing human decision-makers.

5. **We will NOT claim that ArcaScience is "under review by the MHRA" or that MHRA is evaluating ArcaScience for endorsement.** We will describe the engagement accurately: "We are in exploratory discussions with the MHRA about potential utility of structured evidence tools in post-authorization safety assessment."

6. **We will NOT claim "100% accuracy," "100% completeness," or "zero hallucination."** We will provide validated performance metrics with confidence intervals, error rates, and known limitations for each model.

7. **We will NOT use the phrase "World's largest" or any unverifiable superlative** about database size, model count, or capability scope.

8. **We will NOT claim that the system handles the "full drug lifecycle"** without immediately specifying which phases are covered, which data types are included, and which are not (e.g., omics, imaging, real-world data sources like CPRD that cannot be shared).

9. **We will NOT claim that the system "detects insights" or "finds signals" without specifying that these are candidate signals or evidence patterns that require expert validation.** We will never imply that an AI-identified pattern constitutes a clinical finding.

10. **We will NOT cite metrics (60% faster, 9x more insights, 3x DDI boost, 80% time reduction) without providing the baseline, methodology, sample size, and client context** in which they were measured. Uncontextualized metrics will not appear in any regulator-facing material.

11. **We will NOT describe outputs as "generated documents"** (e.g., "generates PSURs"). We will say: "pre-populates structured templates" or "assembles draft evidence summaries for expert review and completion."

12. **We will NOT reference competitive win rates or commercial performance** in any regulator-facing communication. MHRA is not a customer; they are a potential collaboration partner.

13. **We will NOT use the term "black box" to describe competitor approaches without acknowledging that our own model transparency must be demonstrated, not merely asserted.** We will proactively provide auditability evidence rather than claiming non-black-box status.

14. **We will NOT claim that the system works for "any therapeutic area" or "all data types."** We will specify exactly which therapeutic areas have been validated, and which data types are and are not supported (confirming exclusion of imaging, omics, and certain proprietary real-world databases).

15. **We will NOT trivialize the difficulty of regulatory work.** Phrases like "Don't spend months looking for it," "so easy," or "in mere seconds" are permanently banned from regulator-facing vocabulary. We will acknowledge complexity and position the tool as one component of a rigorous process.

---

## 3. Safer Reframing

For each risky claim, the following provides a regulator-appropriate replacement. The replacements follow a consistent principle: lead with what the human does, specify what the system does, and bound the claim with context.

---

### 3.1 Speed Claims

| Original Claim | Safer Replacement |
|----------------|-------------------|
| "What used to take 18 months now completes in seconds" | "The evidence consolidation phase -- gathering, structuring, and cross-referencing published literature and clinical data -- is reduced from weeks to hours, freeing assessor time for the interpretive and judgment work that only qualified experts can perform." |
| "Build your drug's BRE in seconds" | "Rapidly assembles a structured evidence base that assessors use as the starting point for benefit-risk evaluation. The evidence assembly step is accelerated; the evaluation remains a human-led process." |
| "Don't spend months looking for it" | "Reduces the time assessors spend on manual literature search and data extraction, so they can invest more time in critical appraisal and contextual judgment." |
| "18 months of work done in mere seconds" | "Compresses the data ingestion and structuring phase from months to hours. Expert review, validation, and the benefit-risk assessment itself proceed on the assessor's timeline." |
| "60% faster evaluation time" | "In a validated engagement with [client], the evidence-gathering phase was completed 60% faster than the client's prior manual process, as measured by [methodology]. Assessment and review timelines were unchanged." |
| "Reduce BRA Project Time by 80%" | "In [N] client engagements, the data preparation and evidence structuring phases of benefit-risk projects were completed up to 80% faster. This metric covers data ingestion through structured output; it does not include expert review, quality assessment, or final judgment." |

### 3.2 Volume and Scale Claims

| Original Claim | Safer Replacement |
|----------------|-------------------|
| "24 proprietary AI models process millions of data points" | "24 task-specific models -- each trained for a discrete extraction function (e.g., adverse event identification, dosage extraction, study design classification) -- process structured and unstructured data. Each model's performance is validated against clinician-annotated test sets with published precision and recall metrics." |
| "100+ billion data points" | "The platform indexes data from [N] public sources (PubMed, ClinicalTrials.gov, FAERS, VigiBase, etc.) and can integrate client-provided private data. Data volume is less important than data relevance and quality -- the system allows assessors to filter by source type, evidence level, and therapeutic area." |
| "World's largest benefit-risk intelligence database" | "A large-scale, structured evidence repository covering [N] therapeutic areas, integrating public regulatory and clinical data sources. We do not claim comprehensiveness -- assessors define the scope and validate the relevance of included data." |
| "9x more insights detected" | "In blind validation exercises with [N] pharmaceutical clients, the system identified evidence patterns that the client's manual process had not surfaced. In one engagement, 50 additional candidate risk signals were identified beyond the client's original 5 -- each of which required expert review to determine clinical significance." |
| "3x DDI detection boost" | "In validated benchmarks against [baseline method], the system surfaced 3x more candidate drug-drug interaction signals from published literature. All candidate signals require pharmacological review before inclusion in any regulatory assessment." |

### 3.3 Regulatory Acceptance Claims

| Original Claim | Safer Replacement |
|----------------|-------------------|
| "100% regulatory acceptance rate with FDA, EMA, PMDA" | "Outputs from the ArcaScience platform have been incorporated into regulatory submissions by [N] pharmaceutical clients to FDA, EMA, and PMDA. Regulatory acceptance reflects the quality of the sponsor's overall submission, not an endorsement of any individual tool." |
| "50+ regulatory submissions accepted" | "Our pharmaceutical clients have used ArcaScience-supported evidence packages in over 50 regulatory submissions across 12 therapeutic areas." |
| "Currently under review by the MHRA" | **DELETE ENTIRELY.** Replace with: "We are in early-stage exploratory discussions with the MHRA about potential applications of structured evidence tools in post-authorization benefit-risk assessment." If the engagement has not been formally agreed, even this may be too strong -- consult MHRA contacts before any public reference. |
| "ArcaScience won 100% of its deals when facing traditional consulting companies" | **Remove from all regulator-facing materials.** For commercial contexts only: "In competitive evaluations, clients have consistently selected ArcaScience for [specific capability differentiators]." |

### 3.4 Capability and Scope Claims

| Original Claim | Safer Replacement |
|----------------|-------------------|
| "AI-Driven Benefit-Risk Analysis Across the Full Drug Lifecycle" | "AI-supported evidence structuring for benefit-risk assessment, covering pre-clinical through post-authorization phases. The system structures and organizes evidence; the analysis and assessment are performed by qualified human experts." |
| Generates "PSUR/PBRER, Risk Management Plans, CTD 2.5 sections, HEOR reports" | "Pre-populates structured templates for PSUR/PBRER, Risk Management Plans, CTD 2.5 sections, and HEOR reports. All pre-populated content requires expert review, validation, and completion before submission." |
| "47 countries harmonized" | "Regulatory template mappings available for 47 country-specific submission formats, enabling consistent evidence presentation across jurisdictions." |
| "By 2025, 80% of pharma will have adopted AI benefit-risk-enabled solutions" | **Remove.** The prediction is expired and did not materialize. If industry trend data is needed, source a current (2026) analyst report with proper citation. |

---

## 4. Messaging Principles for MHRA Communications

These five principles govern all communications with MHRA and any regulatory authority. They should be printed and reviewed before every meeting, email, or deck revision.

### Principle 1: Always Separate Automation from Judgment

Every statement about system capability must clearly distinguish between:
- **What the system automates:** Data ingestion, structuring, cross-referencing, normalization, template pre-population
- **What the system does NOT automate:** Clinical interpretation, quality assessment of studies, contextual judgment about benefit-risk balance, final decision-making

**Test:** For every claim, ask: "Could an MHRA assessor read this and feel that we are claiming to do their job?" If yes, rewrite.

### Principle 2: Lead with What the Human Does, Not What the AI Does

Wrong: "Our AI identifies 9x more safety signals than manual review."
Right: "Assessors using the platform have access to a broader evidence base, enabling them to evaluate candidate signals that may not surface in manual literature review."

The subject of every sentence about capability should be the human expert, not the AI system. The AI is the instrument; the human is the agent.

### Principle 3: Use Confidence Intervals, Not Absolutes

Wrong: "100% regulatory acceptance rate"
Right: "In N=50+ submissions where our outputs were incorporated, the acceptance rate observed was X% (95% CI: Y%-Z%)."

Wrong: "Zero hallucination"
Right: "Our models are task-specific extractors, not generative language models. Extraction accuracy on validated test sets ranges from [X]% to [Y]% depending on the task, with error analysis available for each model."

Every quantitative claim must include: sample size, baseline, methodology reference, and known limitations.

### Principle 4: Acknowledge Limitations Before Stating Capabilities

In any presentation to MHRA, lead with what the system cannot do:
- "The system does not assess the quality or validity of individual studies -- that remains the assessor's role."
- "The system does not currently handle imaging data or omics data."
- "The system cannot replace domain expertise in contextualizing evidence within a specific national prescribing context."

Only after establishing boundaries should capabilities be presented. This mirrors how regulators themselves think: identify risks first, then evaluate benefits.

### Principle 5: Never Claim to "Replace" or "Automate" Benefit-Risk Assessment

The phrase "benefit-risk assessment" refers to a judgment that carries legal, clinical, and ethical weight. It is performed by qualified professionals who bear personal and institutional accountability for their conclusions.

ArcaScience may claim to:
- **Support** benefit-risk assessment
- **Inform** benefit-risk assessment
- **Accelerate the evidence-gathering phase** of benefit-risk assessment
- **Structure evidence** for use in benefit-risk assessment
- **Reduce manual effort** in the preparatory phases of benefit-risk assessment

ArcaScience may never claim to:
- **Perform** benefit-risk assessment
- **Automate** benefit-risk assessment
- **Complete** benefit-risk assessment
- **Replace** assessors in benefit-risk assessment
- **Deliver** a benefit-risk assessment

### Supplementary Principle 6: Respect the MHRA's Specific Context

The MHRA's post-authorization work involves:
- Highly confidential patient-level data (Yellow Card, CPRD) that cannot leave MHRA systems
- Established drugs with decades of real-world use and complex safety profiles
- Unstructured, noisy, observational data -- not clean clinical trial datasets
- Multiple simultaneous safety issues (80+ at any time) across hundreds of drugs and devices
- Effectiveness (real-world performance), not just efficacy (trial performance)
- UK-specific prescribing patterns, line-of-therapy positioning, and population characteristics

Every claim and capability statement must be tested against this context. If a claim only holds for clean clinical trial data or pre-authorization use cases, it must not be presented as applicable to MHRA's post-authorization work without explicit qualification.

---

## 5. Visual and Diagram Suggestions for MHRA Presentations

The following diagrams are recommended for future MHRA presentations. Each is described in enough detail for a designer to produce. The overarching principle: every diagram should make the human assessor the central figure and the AI system a supporting tool.

---

### 5.1 Human-in-the-Loop Workflow Diagram

**Title:** "Assessor-Led Workflow: Where Automation Ends and Judgment Begins"

**Description:** A horizontal process flow with two swim lanes -- one labeled "System (Automated)" and one labeled "Assessor (Expert Judgment)." The flow proceeds through stages:

1. **Data Ingestion** (System lane): Public data sources (PubMed, FAERS, VigiBase, ClinicalTrials.gov) and client/MHRA-provided data are ingested.
2. **Structuring & Extraction** (System lane): 24 task-specific models extract entities (AEs, endpoints, dosages, study designs, patient demographics).
3. **Cross-referencing & Normalization** (System lane): Extracted data normalized to standard ontologies (MedDRA, ATC, etc.), mapped to knowledge graph.
4. **HANDOFF POINT** (clearly marked with a prominent divider): Structured evidence package delivered to assessor.
5. **Quality Assessment** (Assessor lane): Assessor reviews study quality, confidence intervals, methodology limitations, heterogeneity.
6. **Contextual Judgment** (Assessor lane): Assessor applies UK prescribing context, population-specific considerations, line-of-therapy positioning.
7. **Benefit-Risk Determination** (Assessor lane): Assessor weighs evidence and reaches conclusion.
8. **Regulatory Action** (Assessor lane): Assessor drafts assessment report and recommendations.

**Key design element:** The handoff point should be the visual center of gravity. Steps 1-3 are shown in a muted color (grey/blue); steps 5-8 are shown in a prominent color (MHRA teal). The message: the system does the groundwork; the assessor does the science.

---

### 5.2 Audit Trail Schematic

**Title:** "From Source Document to Extracted Element: Full Traceability"

**Description:** A vertical drill-down diagram showing a single data point's journey:

1. **Source:** A published article (shown as a document icon with DOI/PMID)
2. **Document Classification:** Model identifies article as "observational study, retrospective cohort"
3. **Section Identification:** Model identifies methodology, results, and safety sections
4. **Entity Extraction:** From the safety section, model extracts: "myocardial infarction," "adalimumab," "2 weeks post-administration," "hospitalized"
5. **Normalization:** "myocardial infarction" mapped to MedDRA PT 10028596; "adalimumab" mapped to ATC L04AB04
6. **Contextualization:** Study metadata attached: N=450, retrospective, single-center, adults 18-65, no placebo arm
7. **Assessor Review Point:** Extracted element presented to assessor with full provenance chain. Assessor can click through to source document, verify extraction, accept/reject/modify.

**Key design element:** Every arrow in the chain is bidirectional (assessor can trace back). A "confidence score" is shown at each extraction step. The final element is flagged as "Candidate -- Pending Expert Review," never as "Finding" or "Signal."

---

### 5.3 Data Quality and Limitations Transparency Panel

**Title:** "What the System Knows, What It Does Not Know, and What Only You Know"

**Description:** A three-column panel:

| What the System Does | What the System Flags But Cannot Judge | What Only the Assessor Knows |
|----------------------|----------------------------------------|------------------------------|
| Extracts structured data from unstructured sources | Study sample size, design type, follow-up duration (presented without quality judgment) | Whether the study methodology is adequate for the question being asked |
| Normalizes terminology to standard ontologies | Data gaps and missing fields (flagged as incomplete) | Whether a data gap is clinically meaningful |
| Cross-references findings across sources by mechanism of action | Conflicting evidence between sources (presented side by side) | How to weigh conflicting evidence in context |
| Identifies candidate safety signals by frequency and co-occurrence | Statistical measures (incidence rates, confidence intervals where reported) | Whether an association is causal |
| Pre-populates template sections | Template sections where no evidence was found (flagged as empty) | What additional sources or expert consultation are needed |

**Key design element:** The "What Only the Assessor Knows" column should be the widest and most prominent. This communicates that the system's contribution is necessary but not sufficient.

---

### 5.4 Post-Authorization Use Case Flow

**Title:** "Supporting Post-Authorization Safety Assessment: A Practical Workflow"

**Description:** Tailored specifically to MHRA's post-authorization context (addressing Allison's concern that she could not see how the tool applies to her work):

1. **Safety Signal Emerges:** MHRA identifies a potential safety concern through Yellow Card, EudraVigilance, or external report (this step is entirely outside the system).
2. **Evidence Scoping (Assessor-defined):** Assessor specifies the drug class, adverse event of interest, and relevant data sources. System is configured to search only within assessor-defined parameters.
3. **Literature Sweep (System):** System retrieves and structures all published literature matching the query -- observational studies, meta-analyses, case series, systematic reviews. Each article is classified by study type, sample size, and reported outcomes.
4. **Comparator Assembly (System):** System pulls structured data on the drug class -- all drugs with same ATC code, similar mechanisms, or overlapping indications. Comparative safety profiles are assembled.
5. **Evidence Dashboard (Delivered to Assessor):** Assessor receives a structured evidence package: number of studies found, study types, key findings, data gaps, conflicting results. Everything is hyperlinked to source.
6. **Critical Appraisal (Assessor):** Assessor evaluates study quality, applicability to UK population, relevance to the specific safety question. Accepts, rejects, or annotates each evidence element.
7. **Assessment Report Drafting (Collaborative):** System pre-populates relevant template sections with assessor-approved evidence. Assessor writes analysis, conclusions, and recommendations.

**Key design element:** Steps 1 and 6-7 are shown entirely in the "Assessor" swim lane. The system contributes only in steps 3-5. Step 2 (scoping) is shown as a joint activity where the assessor directs and the system executes.

---

### 5.5 Model Architecture Transparency Diagram

**Title:** "24 Task-Specific Models: What Each One Does and Does Not Do"

**Description:** A grid or matrix showing each of the 24 models (or representative categories) with:

- **Model function:** e.g., "Study Type Classifier," "Adverse Event Extractor," "Dosage Normalizer"
- **Input:** What the model receives (e.g., full-text article, methodology section only)
- **Output:** What the model produces (e.g., structured label: "RCT, double-blind, N=450")
- **What it does NOT do:** Explicit statement (e.g., "Does not assess study quality or risk of bias")
- **Validation metric:** Precision/recall on annotated test set (e.g., "Precision: 94%, Recall: 91% on N=2,000 annotated articles")
- **Error handling:** What happens when the model is uncertain (e.g., "Low-confidence extractions flagged for manual review")

**Key design element:** The "What it does NOT do" column should be as prominent as the "Model function" column. This preemptively addresses Allison's question about validation and error propagation through chained models.

---

### 5.6 Comparison: "What Changes vs. What Stays the Same"

**Title:** "With ArcaScience, Here Is What Changes in Your Workflow -- and What Does Not"

**Description:** A simple two-column layout:

| What Changes (Faster, More Structured) | What Stays the Same (Assessor-Owned) |
|-----------------------------------------|--------------------------------------|
| Literature search and retrieval | Study quality assessment |
| Data extraction from unstructured sources | Clinical interpretation of findings |
| Terminology normalization | Contextual judgment (UK prescribing patterns, population) |
| Cross-referencing across data sources | Causality determination |
| Template pre-population | Benefit-risk weighing and conclusion |
| Comparator drug profiling | Regulatory recommendation and action |
| Evidence gap identification | Decision on additional data needs |

**Key design element:** The right column ("What Stays the Same") should be visually equal or larger than the left column. The message: the assessor's expertise is not diminished; their time is redirected from manual labor to expert judgment.

---

## Appendix: Priority Actions Before Next MHRA Contact

1. **IMMEDIATE:** Remove "currently under review by the MHRA" from Deck 2026. This is a trust-destroying claim.
2. **IMMEDIATE:** Remove all "in seconds" language from any material that could be seen by MHRA, including the live demo environment.
3. **BEFORE NEXT MEETING:** Rewrite the deck opening slide to lead with what the assessor does, not what the AI does.
4. **BEFORE NEXT MEETING:** Prepare a one-page "Limitations and Boundaries" document that proactively states what the system cannot do.
5. **BEFORE NEXT MEETING:** Prepare validated performance metrics (precision, recall, F1) for each model, with test set sizes and annotation methodology.
6. **BEFORE NEXT MEETING:** Prepare a post-authorization-specific use case (using publicly available safety data) that demonstrates the workflow Allison described: noisy, observational, multi-source, UK-relevant.
7. **ONGOING:** Establish an internal review process: every slide, email, and talking point for MHRA must pass through the "Claims We Will NOT Make" checklist before use.

---

*Document prepared by Agent 5 -- Evidence & Website Miner*
*For internal use in MHRA meeting response preparation*
*All claims sourced from arcascience.ai, arcascienceval.live, ArcaScience Deck 2026, and MHRA meeting transcript*
