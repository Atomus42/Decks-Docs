# MHRA Question Matrix -- Full Traceability Table

**Deliverable C: ArcaScience MHRA Collaboration Preparation**
**Prepared by: Agent 7 (Editor / Deck Strategist)**
**Date: 2026-02-15**
**Classification: For internal preparation only**

---

## How to Read This Document

This matrix captures all 50 questions and concerns raised by MHRA participants during the introductory meeting. Each row traces a single concern from its origin in the transcript through to our proposed response, supporting evidence, and planned demonstration artifact.

**Column definitions:**

| Column | Description |
|--------|-------------|
| **Timestamp / Speaker** | When and who raised the concern in the transcript |
| **Concern / Question** | Verbatim or near-verbatim quote from the meeting |
| **Why It Matters to MHRA** | The regulatory or operational significance of this concern |
| **Our Response** | Precise, regulator-appropriate answer synthesised from scratch files. Clearly separates what the system does (extraction/structuring) from what the assessor does (judgment) |
| **Evidence Pointer** | Source documents, scratch files, ArcaScience materials, or transcript lines supporting the response |
| **Demo Artifact** | What we will show in the follow-up meeting or PoC to address this concern |
| **Status** | Ready = response is fully supported by documented capabilities. TBD = response requires confirmation, engineering work, or information not yet documented |

**Themes:**
- [A] Post-Authorisation Applicability (Questions 1, 10, 11, 12, 15, 21, 22, 23, 24, 26, 49, 50)
- [B] Model Validation and Auditability (Questions 3, 5, 7, 9, 14, 25, 34, 35, 37, 38, 48)
- [C] Scalability (Questions 6, 11, 13, 41, 42)
- [D] Confidentiality (Questions 28, 31, 32, 39, 43)
- [E] Data Sources and Quality (Questions 2, 8, 18, 19, 29, 33, 36)
- [F] UK Context (Questions 16, 17)
- [G] Messaging (Questions 4, 20)
- [H] Collaboration Terms (Questions 27, 30, 40, 44, 45, 46, 47)

---

## Theme A: Post-Authorisation Applicability

These concerns represent the single largest blocker identified in the meeting. Allison stated unequivocally: "How do you solve MY problem rather than THIS problem?" Every response must demonstrate direct relevance to post-authorisation safety assessment.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 1 | 04:18 -- Steph | "A lot of what we do is managing those risks. And they're not obviously as I said earlier, looking at safety post authorization. So that's kind of where we would be looking to use something like this." | Steph signals that ArcaScience's pitch is oriented toward pre-authorisation, but MHRA's primary need is post-authorisation safety management. If the tool has no post-auth utility, it is not relevant to their work. | The system's core function -- extracting, structuring, and cross-referencing evidence from published literature and public data sources -- applies directly to the evidence assembly step (Step 3) of MHRA's post-authorisation workflow. The system ingests observational studies, meta-analyses, case reports, safety communications, and regulatory assessments. The assessor uses the structured output to perform critical appraisal and benefit-risk judgment. The system does not make judgments; it organises the evidence base the assessor works from. | 02_mhra_workflow.md Sections 5a-5c (evidence assembly, organisation, completeness check); 03_under_the_hood.md Section 6 (extraction vs. judgment boundary); 06_poc_plan.md Section 2 (post-auth PoC design) | PoC on a completed post-auth safety issue (SGLT2/DKA recommended) using only public-domain data. Demonstrate pipeline on observational studies and regulatory safety communications, not clinical trials. | Ready |
| 10 | 23:23 -- Allison | "So I am struggling to understand how we would use this in the post authorization space. I can sort of see how you might use it in that context because you've got structured data coming in from clinical trials... And you build bespoke models for every use case effectively." | Allison explicitly states she does NOT see post-authorisation utility. Pre-auth has structured, high-quality data; post-auth has messy, unstructured, diverse data. This is the central credibility gap. | We acknowledge that our initial presentation emphasised pre-authorisation use cases, which was a positioning error on our part. The system's extraction models are trained on 10,000+ documents spanning all clinical phases, including observational studies and post-marketing publications. The models extract from unstructured text regardless of whether the source is a clinical trial report or a published observational cohort study. The pipeline does not require structured input data -- it creates structure from unstructured sources. The assessor then applies their expertise to evaluate the quality and relevance of the structured output. | 03_under_the_hood.md Section 2 (task-specific models, cross-therapeutic training data); 02_mhra_workflow.md Section 5a (evidence assembly from literature); 01_quotes.md Theme 1 analysis | Live demo: ingest an unstructured observational study from the SGLT2/DKA literature. Show structured extraction of study design, endpoints, confounders, CIs, and limitations -- all traceable to source paragraphs. | Ready |
| 12 | 24:15 -- Allison | "I want to build a benefit risk summary on the risk of antidepressants and persistent sexual dysfunction. So nothing in the clinical trials... everything is observational data, anecdotal reports, yellow card reports, things in the media, discussion with patients." | Allison provides a concrete test case. It involves a post-auth scenario where clinical trial data is inadequate (only 12-week trials) and the evidence base is heterogeneous and low-quality. The platform must handle this type of messy, real-world data or it has no utility for MHRA. | This is exactly the type of scenario the PoC is designed to address. Using only public-domain sources (published observational studies, case series, regulatory safety communications, systematic reviews), the system would extract and structure the available evidence. The system would surface what it finds and explicitly flag gaps -- for example, the absence of long-term clinical trial data, the limitation that Yellow Card data is not accessible, and the reliance on observational associations rather than controlled evidence. The assessor receives a structured evidence map with confidence indicators and gap analysis. The assessor -- not the system -- determines whether the evidence supports a causal association. | 06_poc_plan.md Section 1 (PoC selection criteria match this pattern); 04_data_governance.md Section 2 Tier 1 (public data sources); 03_under_the_hood.md Section 6 (system does not judge causality) | If MHRA selects the antidepressant/PSSD case for the PoC, run it. Otherwise, demonstrate the SGLT2/DKA case which has comparable characteristics (post-auth, observational evidence, causality complexity). | Ready |
| 15 | 26:31 -- Allison | "But we don't do efficacy, of course, we do effectiveness... can't necessarily take efficacy or effectiveness from clinical trial and assume that it translates into performance on the market." | MHRA works with real-world effectiveness, not clinical trial efficacy. The platform must handle this distinction. | The system extracts efficacy endpoints from clinical trials and effectiveness data from observational studies as separate evidence categories. The system does not conflate them. When structuring evidence, it tags each finding with its study type (RCT, observational cohort, case series, etc.) and its evidence level, allowing the assessor to distinguish trial-derived efficacy from real-world effectiveness. The system does not translate efficacy into effectiveness -- that contextual judgment is the assessor's domain. | 03_under_the_hood.md Section 6 (extraction categories include study type tagging); 02_mhra_workflow.md Section 3c (effectiveness vs. efficacy distinction) | Show the extraction output for the same drug from an RCT vs. an observational study, with clear labelling of evidence type. Demonstrate that the system does not merge or equate the two. | Ready |
| 21 | 34:43 -- Allison | "I think that those are much easier questions than for drugs that have been on the market for a long time. And safety events emerge." | The Sanofi rare disease example is not representative of MHRA's hardest problems. Long-marketed drugs with emerging safety signals are far more complex. | We agree. The PoC is designed specifically to address a long-marketed drug class. The recommended candidate (SGLT2 inhibitors, on market since 2012-2014) involves a post-marketing safety signal (DKA) that emerged years after authorisation. The backup candidate (fluoroquinolones, on market since the 1980s) involves signals that accumulated over 30+ years. The system's value is in comprehensively assembling and structuring the accumulated evidence across decades of publications. The assessor brings the clinical expertise to evaluate that evidence in context. | 06_poc_plan.md Section 2 (all three PoC options involve long-marketed drugs); 06_poc_plan.md Appendix A (drug class age comparison) | PoC deliverable: temporal evidence map showing signal evolution across years of post-marketing exposure. | Ready |
| 22 | 35:04 -- Allison | "You've got a lot of people using them and you may have a rare event. And you're trying to understand causality of that event across a lot of different data sources." | Establishing causality for rare events in widely-used drugs across heterogeneous data sources is the core post-auth challenge. The platform must support causal reasoning, not just data aggregation. | The system does not determine causality -- that is a scientific judgment requiring pharmacological expertise, biological plausibility assessment, and contextual reasoning that only the assessor can perform. What the system does: it extracts and structures the evidence elements that feed causal reasoning -- temporality, dose-response data, dechallenge/rechallenge outcomes, confounders assessed, and consistency of findings across sources. The system presents these elements organised and cross-referenced so the assessor can evaluate the causal case systematically rather than assembling it manually from dozens of publications. | 02_mhra_workflow.md Section 6c (causality determination excluded from system capabilities); 03_under_the_hood.md Section 6 (what system does NOT do); 05_positioning_findings.md Principle 1 (separate automation from judgment) | Demo: show extraction of Bradford Hill-relevant evidence elements (temporality, dose-response, consistency, specificity) from multiple publications on the same drug-event pair, presented in a structured comparison table. | TBD -- extraction of Bradford Hill elements not explicitly documented as a current pipeline feature |
| 23 | 36:23 -- Allison | "That's what I want you to solve." | Direct statement: solve the POST-authorisation problem, not the pre-authorisation one. This is the clearest possible signal of what the follow-up must focus on. | Understood. Every element of our follow-up is anchored in post-authorisation safety assessment. The PoC uses a completed post-authorisation safety issue. The demo shows post-marketing evidence types (observational studies, safety communications, spontaneous reporting aggregates). We will not present pre-authorisation use cases to MHRA. | 06_poc_plan.md Design Principles (#4: post-authorisation focus); 01_quotes.md Theme 1 (post-auth as dominant theme) | Entire PoC is post-auth-focused. No pre-auth examples in any MHRA-facing materials. | Ready |
| 24 | 36:28 -- Allison | "I'm not interested... you're talking to a load of post authorization people here... welcome to the world of post authorization, which is a million times worse. So how do you solve my problem rather than this problem?" | The most forceful statement in the meeting. ArcaScience must stop pitching pre-auth and anchor entirely in post-authorisation scenarios. | We hear this clearly. Our follow-up repositions the entire value proposition: the system accelerates the evidence assembly phase of post-authorisation safety assessment by extracting and structuring findings from the messy, heterogeneous data sources that post-auth assessors rely on -- published observational studies, meta-analyses, case series, aggregate spontaneous reporting data, and regulatory safety communications. The assessor retains full authority over quality assessment, contextual judgment, and the benefit-risk determination. | 05_positioning_findings.md Section 3.4 (safer reframing of capability claims); 02_mhra_workflow.md Section 5 (where ArcaScience could help); 06_poc_plan.md Section 0 (#4: post-auth focus) | Reframed opening slide: "AI-supported evidence structuring for post-authorisation safety assessment." No pre-auth content in any MHRA-facing material. | Ready |
| 26 | 39:18 -- Allison | "In order to invest our time in something like this, I need to be confident it has utility in the space that we're working at, and I probably need a deeper dive, and I can't see how we could use it in our day-to-day work." | Time investment justification: Allison needs demonstrated utility in actual daily work before committing resources. The PoC must map directly to a real MHRA workflow. | The PoC is designed to map directly to Step 3 (evidence assembly) and the transition to Step 4 (critical appraisal) of the MHRA post-authorisation workflow. ArcaScience runs its pipeline against public data for a safety issue MHRA has already assessed. MHRA compares the output against their own prior work. If the output covers what their assessors found and surfaces it in a structured, traceable format, that is demonstrated utility for the evidence-gathering phase. If it also identifies additional relevant evidence the original assessment did not include, that is incremental value. The assessor's interpretive work remains unchanged. | 02_mhra_workflow.md Section 1 (workflow steps); 06_poc_plan.md Section 3 (success metrics); 06_poc_plan.md Section 4 (deliverables) | PoC deliverables: evidence map, structured extraction, gap analysis, traceability report -- all directly mapped to the MHRA workflow. | Ready |
| 49 | 30:26 -- Steph | "So for a brand new innovative product at the point for authorization, you would have very clear what the benefits and risks are. And then the model would add more information to that as there were more studies published... it would be almost a repository of information for us to use in our assessment." | Steph envisions the tool as a living repository that accumulates evidence over a product's lifecycle, continuously incorporating new publications and data. | This longitudinal use case aligns with the system's architecture. The Profiling Base is continuously updated with newly indexed public literature. For a given drug or safety question, the evidence map grows over time as new studies are published and ingested. The system structures each new piece of evidence using the same extraction pipeline, adding it to the existing knowledge graph. The assessor can review the evolving evidence landscape at any point. The system does not re-assess -- it re-structures. The assessor determines whether new evidence changes the benefit-risk picture. | 03_under_the_hood.md Section 1 (Profiling Base with 100B+ data points, continuously indexed); 02_mhra_workflow.md Section 5b (evidence organisation) | Show a time-stamped evidence map that demonstrates how the system tracks the accumulation of evidence over multiple years for a single safety question. | TBD -- longitudinal evidence tracking demo not yet confirmed as ready for live demonstration |
| 50 | 03:44 -- Steph | "I've looked at it... latest version. And it was interesting. And I think in terms of facilitating some of our assessment, it offers utility." | Steph provides the most positive MHRA feedback in the meeting. She is the most receptive participant and has independently explored the beta platform. Build on this qualified endorsement. | We appreciate Steph's engagement with the platform. We will ensure the beta environment is configured to support post-authorisation exploration, with example outputs from observational studies and post-marketing data types. We will provide Steph-specific guidance on navigating the beta for post-auth use cases, including where to find traceability links and source provenance for extracted data. | 01_quotes.md Speaker Summary (Steph as most receptive); 06_poc_plan.md Section 6 (beta access continuation) | Ensure beta platform has post-auth-relevant example content. Provide navigation guide for MHRA assessors exploring the beta independently. | TBD -- beta configuration for post-auth content needs confirmation |

---

## Theme B: Model Validation and Auditability

Allison and Steph both raised fundamental questions about how the AI models work, how they are validated, how errors are traced, and whether the system is a "black box." These concerns must be addressed with technical transparency, not marketing language.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 3 | 05:11 -- Steph | "I guess I'm asking about the black box of that AI tool and what's it based on, because that's fundamental for us to understand that." | Steph perceives the platform as a black box that makes judgments. She needs to understand the basis for outputs. If the reasoning is not auditable, MHRA cannot rely on it. | The system is not a black box. It operates as a chain of 24 task-specific small language models, each performing a single, well-defined extraction function. Each step produces inspectable intermediate output before feeding the next step. The assessor can trace any extracted data element back through the chain: which model produced it, from which section of which source document, and with what confidence score. The system does not produce free-text generated conclusions (unlike ChatGPT). It produces structured, templated extractions where every element is linked to a source. | 03_under_the_hood.md Section 1 (pipeline diagram); Section 3 (traceability mechanisms); Section 4 (error localisation); Transcript: Romain -- "The information that comes in and comes out is auditable at both ends" | Audit trail walkthrough: select any data point in the output, trace it back through extraction step, section identification, document classification, to original source paragraph. Live click-through in demo. | Ready |
| 5 | 12:20 -- Allison | "24 small AI models trained by clinicians. Okay. Well, I mean, how can you do that then? I just don't understand how you can do that." | Allison fundamentally does not understand the technical architecture and finds the claim of 24 small models implausible. She needs a clear, credible explanation. | Each of the 24 models is trained for one specific task -- for example, one model classifies document type (clinical trial vs. observational study vs. case report), another extracts adverse event terms, another identifies study design elements (sample size, duration, blinding), another normalises extracted terms to standard ontologies (MedDRA, SNOMED CT). "Trained by clinicians" means the training data is annotated by two independent clinicians following structured annotation guidelines, and model performance is validated against this clinician-annotated gold standard. The models are small because each does one narrow task well, rather than one large model attempting everything. | 03_under_the_hood.md Section 2 (task-specific model table with categories and descriptions); Section 5 (validation approach -- clinician-annotated test sets) | Model cards: present a one-page summary for each model category showing input, output, what it does, what it does NOT do, and validation metrics. Visual architecture diagram (see 05_positioning_findings.md Section 5.5). | Ready |
| 7 | 13:07 -- Allison | "When you've got such complex models that they're happening in series, how do you validate them? How do you identify where that error might have happened?" | With 24 models in series, errors can compound and become untraceable. Allison needs to know how each model is independently validated and how errors are traced through the chain. This is a core regulatory requirement for any tool they would adopt. | Each model is validated independently using clinician-annotated test sets. Error metrics (missed data, miscategorised data, clustering errors) are measured per model, per task. Because each step produces inspectable intermediate output, an error can be localised to the specific step where it occurred -- e.g., if an adverse event is miscategorised, you can verify that the document classification and section identification were correct and that the error occurred specifically in the entity extraction step. The F1 target for extraction models is >= 85%. | 03_under_the_hood.md Section 4 (error localisation with step-by-step example); Section 5 (four validation methods); Section 5 metrics table (F1 >= 85% target) | Step-by-step error trace: introduce a known error in the demo and show how the auditable intermediate outputs allow localisation to the specific model that produced it. | TBD -- formal error propagation analysis across the full chain is not yet documented |
| 9 | 18:42 -- Sharinto | "Does it come up with levels of uncertainty, or does it flag where there are issues? For example, because of gaps in information source." | The system must not present incomplete data as complete. It must surface confidence levels and flag data gaps transparently to the assessor. | The system produces gap analysis as a standard deliverable: evidence types that should exist but were not found, geographic gaps (e.g., no UK-specific data), and temporal gaps. For confidence scoring: per-insight confidence levels, disagreement flags (where sources conflict), and missing evidence indicators are described in the OKR roadmap as planned features. Currently, source-to-statement linkage and per-step auditability are operational. The assessor can see what evidence was found and from where; the gap analysis shows what was not found. | 03_under_the_hood.md Section 3 (operational vs. planned table -- confidence scoring is planned, source linkage is operational); 06_poc_plan.md Section 4.5 (confidence scoring deliverable); Section 4.6 (gap analysis deliverable) | PoC deliverable includes gap analysis document. Confidence scoring will be demonstrated at the level currently available. Planned features will be described honestly as in-development. | TBD -- per-insight confidence scoring is planned, not yet operational |
| 14 | 25:44 -- Allison | "Could it do it today without, or would you need to build a model for it?" | Direct litmus test: can the system answer an ad hoc post-authorisation question TODAY without new model development? If the answer is "we need to build first," it confirms Allison's scalability fear. | The system can process an ad hoc safety question today using the existing 24 task-specific models. No new model is built. The pipeline configuration -- selecting which data sources to query and which extraction outputs to enable -- takes hours, not weeks. For the PoC, ArcaScience will configure the pipeline for MHRA's chosen safety issue and run it using the existing generic models, demonstrating that no bespoke model development is required. | 03_under_the_hood.md Section 2 ("No retraining needed" for new therapeutic areas); 06_poc_plan.md Design Principle #3 (not bespoke) | Run the pipeline on MHRA's chosen safety issue in real time during the demo meeting, showing that it processes the question using existing models with configuration-only setup. | Ready |
| 25 | 38:54 -- Allison | "I need to see under the hood, really because I'm still a bit skeptical, and you haven't convinced me yet that this could... we could use this in our work, and I'm really keen to look for ways to create efficiency." | Allison wants a deep technical dive, not high-level marketing. She is skeptical but explicitly open-minded ("really keen to look for ways to create efficiency"). The follow-up must provide transparent technical detail. | The follow-up meeting devotes 15 minutes to a dedicated "under-the-hood" walkthrough covering: (a) pipeline architecture with each step diagrammed, (b) model cards for each model category, (c) validation methodology and metrics, (d) error traceability mechanism, (e) what the system does vs. what it does NOT do. No marketing slides. Technical depth only. This is followed by 20 minutes of live demo where MHRA can challenge specific extractions and trace them to source. | 03_under_the_hood.md (entire document prepared for this purpose); 06_poc_plan.md Section 6 (meeting agenda: 15 min under-the-hood walkthrough + 20 min live demo) | Architecture diagram, model cards, validation metrics table, live audit trail demo. All prepared as technical reference documents, not slide decks. | Ready |
| 34 | 44:01 -- Allison | "What my assessors do is they look at the methodology, the confidence intervals, the patient populations, the heterogeneity of the data sources, and they reach an opinion about whether that article is valid and should be included." | The platform must go beyond surface-level data extraction to support critical appraisal: methodology assessment, CI evaluation, population analysis, heterogeneity assessment, and study validity determination. If it only extracts headlines, it does not meaningfully assist the assessor's work. | The system extracts the following critical appraisal elements when present in the source: study design, sample size, follow-up duration, patient population description, inclusion/exclusion criteria, statistical methods, effect sizes with confidence intervals, confounders assessed, limitations acknowledged by authors, and funding/COI disclosures. It does not judge whether these elements are adequate -- that is the assessor's role. The system surfaces the raw materials of critical appraisal in a structured, comparable format so the assessor can perform their evaluation systematically across many studies rather than reading and tabulating manually. | 03_under_the_hood.md Section 6 (extraction categories table); 06_poc_plan.md Section 4.2 (structured extraction output specification); Section 3.1 (critical appraisal coverage metric >= 80%) | Show structured extraction from 3-5 studies on the same safety question, with critical appraisal elements presented side by side in a comparison table. Assessor can immediately see which studies have larger samples, longer follow-up, or more confounders assessed. | Ready |
| 35 | 44:45 -- Allison | "I'm not clear on how you would handle the published literature within your models." | After hearing the explanation of 24 models, Allison still does not understand how published literature is processed. The technical explanation was not persuasive. A concrete, step-by-step demonstration is needed. | The follow-up meeting includes a live, step-by-step demonstration of a single published article being processed: (1) document ingested as PDF, (2) classified as observational study, (3) sections identified (abstract, methods, results, discussion), (4) entities extracted from each section (adverse events, drug names, dosages, patient demographics, study endpoints), (5) entities normalised to MedDRA/SNOMED CT, (6) relations extracted (which drug linked to which event), (7) output presented in structured template with source links. Each step's intermediate output is shown. | 03_under_the_hood.md Section 1 (full pipeline diagram); Section 4 (step-by-step walkthrough); 06_poc_plan.md Section 6 (agenda item: live demo) | Live demo: single observational study processed end-to-end with all intermediate outputs visible. Assessors can pause at any step and inspect. | Ready |
| 37 | 47:40 -- Allison | "How does your model handle those sort of work out?" (referring to observational studies and meta-analyses) | Direct question about handling observational studies and meta-analyses -- the data types that matter most for post-auth work. Not adequately answered in the meeting. | The document classification model identifies meta-analyses and observational studies as distinct document types. For meta-analyses, the extraction pipeline pulls: included study count, search strategy scope, heterogeneity measures (I-squared where reported), pooled effect estimates with CIs, subgroup analyses, and authors' conclusions. For observational studies, it extracts: cohort definition, exposure/comparator groups, follow-up duration, outcome definitions, confounders adjusted for, and effect sizes. These are presented as structured data, not free-text summaries. The assessor evaluates the quality of the meta-analysis or study; the system provides the structured elements for that evaluation. | 03_under_the_hood.md Section 2 (document type classification handles observational studies); Section 6 (extraction categories); 06_poc_plan.md Section 4.2 (structured extraction includes study design elements) | Process a published meta-analysis and a published observational cohort study side by side in the demo. Show that the extraction handles both document types with appropriate extraction templates. | TBD -- meta-analysis-specific extraction (I-squared, forest plot data) needs confirmation as a current capability |
| 38 | 50:00 -- Allison | "Okay, that's extracting information into a structured form for you. It doesn't tell you about the quality of that study. Still." | After Charbel's detailed explanation, Allison identifies the fundamental gap: extraction without quality assessment is insufficient. MHRA assessors need to know whether a study's conclusions are reliable, not just what the study says. | We agree with Allison's assessment: the system extracts data, it does not judge study quality. We are transparent about this boundary. What the system provides is the structured evidence elements that enable efficient quality assessment by the assessor: sample size, study duration, blinding status, confounders assessed, statistical methods, CI width, limitations section content. The system surfaces what is present and flags what is absent (e.g., "no confounder adjustment reported"). The quality judgment -- whether a study with N=50 and no confounder adjustment is reliable -- remains entirely the assessor's determination. We position this as reducing manual extraction time so assessors can invest more time in the interpretive work that only they can perform. | 03_under_the_hood.md Section 6 ("What the system does NOT do" -- does not judge study quality); 05_positioning_findings.md Principle 1 (separate automation from judgment); 05_positioning_findings.md Section 5.3 (transparency panel: "What the System Flags But Cannot Judge") | Present the "What the System Does / What It Flags But Cannot Judge / What Only the Assessor Knows" three-column panel (from 05_positioning_findings.md Section 5.3). Make the boundary explicit and visual. | Ready |
| 48 | 29:01 -- Steph | "It would draw from different sources and summarize that, tell you what your benefits and your risks are and then I think it would be up to us to use that in forming a benefit risk assessment in the same way that we would now. Is that exactly how it's intended to work?" | Steph seeks confirmation that the platform is an information aggregation and stratification tool, NOT a decision-making tool. The human assessor retains full judgment. This framing is more acceptable to MHRA than any suggestion of automated assessment. | That is exactly how it is intended to work. The system aggregates, structures, and stratifies evidence from multiple sources. It presents the assessor with an organised evidence base -- benefits and risks extracted and categorised from the available literature. The assessor then uses that structured evidence to form their benefit-risk assessment, applying their expertise, contextual knowledge, and regulatory judgment. The system does not produce a benefit-risk conclusion. It produces the evidence package that the assessor works from. | 03_under_the_hood.md Section 6 (explicit "human-in-the-loop requirement"); 05_positioning_findings.md Principle 2 ("Lead with what the human does"); 05_positioning_findings.md Section 5.1 (human-in-the-loop workflow diagram) | Present the human-in-the-loop workflow diagram with clear handoff point between system output and assessor judgment (05_positioning_findings.md Section 5.1). | Ready |

---

## Theme C: Scalability

MHRA operates at a fundamentally different scale from a pharmaceutical client. 100 assessors, 600+ drugs, hundreds of thousands of devices, 80 concurrent safety issues. Any tool must work generically without bespoke setup per use case.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 6 | 12:32 -- Allison | "We have a 100 benefit risk assessors, looking across 600 drugs and millions of thousands and hundreds of thousands of devices. Our questions are not always the same." | MHRA's scale is fundamentally different from a pharma client. 100 assessors, 600+ drugs, diverse and unpredictable questions. A bespoke-per-use-case approach is operationally impossible. | The system's architecture is designed for this. The 24 task-specific models are generic -- they extract adverse events, study designs, drug dosages, and patient demographics regardless of therapeutic area. An assessor investigating an antidepressant safety question uses the same models as one investigating a diabetes drug. The only per-question configuration is: (a) which data sources to query, (b) which drug/event terms to focus on, and (c) which output template to use. This configuration is comparable to defining a search strategy -- it takes hours, not weeks, and requires no model retraining. | 03_under_the_hood.md Section 2 (task-specific, not therapeutic-area-specific); 02_mhra_workflow.md Section 7 table (customisation tolerance comparison) | Demonstrate the same pipeline running on two different therapeutic areas with configuration-only changes. Show that no model is retrained between runs. | Ready |
| 11 | 23:56 -- Allison | "And bespoke data sources for every use case... am really struggling to see how we could use it in our work. Because we couldn't build a bespoke model for every use case." | The bespoke-per-use-case perception is a dealbreaker. MHRA needs something that works generically across use cases without custom setup each time. This concern is closely linked to #10 (post-auth applicability). | The 24 models are task-specific (e.g., "extract adverse events," "classify study design," "normalise to MedDRA"), not therapeutic-area-specific. No new model is trained for a new drug or safety question. Adaptation happens through pipeline configuration: selecting which data sources to query and which output templates to use. The same pipeline that processes oncology data also processes cardiology or psychiatry data. The assessor defines the safety question; the system executes extraction across the relevant literature. | 03_under_the_hood.md Section 2 (task-specific vs. therapeutic-area models; "No retraining needed") | Show the same extraction pipeline applied to two different therapeutic areas without model changes. Configuration-only adaptation demonstrated live. | Ready |
| 13 | 24:58 -- Allison | "What I can't see working is for every specific use case, I have to build a bespoke model." | Hard constraint: the solution must NOT require bespoke model building per use case. This is the third time Allison has made this point -- it is emphatic and non-negotiable. | Confirmed: no bespoke model is built per use case. The 24 models are pre-trained and operate across all therapeutic areas. For each new safety question, the assessor or ArcaScience configures the pipeline by specifying the drug, the safety concern, and the data sources. This is analogous to defining a search protocol -- not building a new search engine. The PoC will demonstrate this explicitly: ArcaScience runs its existing pipeline on MHRA's chosen safety issue without training any new model. | 03_under_the_hood.md Section 2 ("No retraining needed"); 06_poc_plan.md Design Principle #3 ("Not bespoke -- existing pipeline and generic algorithms") | PoC explicitly demonstrates generic pipeline, not a custom build. Document pipeline configuration steps to show the effort required (hours, not weeks). | Ready |
| 41 | 53:09 -- Allison | "There might be 80 safety issues ongoing at any one time across medicines and medical devices, and we can't develop an AI model for every use case." | 80 concurrent safety issues means 80 concurrent use cases. Developing or configuring AI models for each is not operationally feasible. The solution must support parallel, diverse use cases with minimal per-case effort. | With 80 concurrent safety issues, the system would run 80 concurrent pipeline configurations -- each targeting different drugs and events but using the same underlying 24 models. This is a computational scaling question, not a model development question. Each configuration is lightweight (query parameters and output template selection). The Kubernetes-based infrastructure supports concurrent pipeline execution. However, the operational model for MHRA managing 80 concurrent configurations would need to be scoped in a subsequent phase -- the PoC demonstrates the principle on a single case. | 03_under_the_hood.md Section 7 (Kubernetes, 100% containerised infrastructure); Section 2 (same models across all areas); 02_mhra_workflow.md Section 4.1 (scale vs. specificity challenge) | PoC demonstrates the principle (generic pipeline, no per-case model). Scaling to 80 concurrent configurations would be addressed in a Phase 2 technical scoping discussion. | TBD -- concurrent multi-issue operational model not yet scoped |
| 42 | 53:36 -- Allison | "How could you give out something generic? It's beyond the literature search because I can get ChatGPT to do that. Beyond that, that would support the assessors, but doesn't require bespoke models." | Two critical points: (1) the platform must offer something BEYOND what ChatGPT/Copilot can do for literature search; (2) it must work generically without bespoke model generation. The differentiation from general-purpose AI tools is the core value proposition question. | The differentiation from ChatGPT is structural, not just qualitative. ChatGPT performs text summarisation -- it reads a paper and tells you what it says. ArcaScience performs structured extraction -- it reads a paper and produces a standardised data record (study design: observational cohort; N=12,400; follow-up: 3.2 years; primary outcome: DKA incidence; HR: 2.1; 95% CI: 1.4-3.2; confounders adjusted: age, BMI, insulin use). Every extracted element is traceable to a specific paragraph in the source. ChatGPT cannot do this reliably, consistently, or with traceability. Additionally, the system cross-references extracted elements across sources using normalised ontologies (MedDRA, SNOMED CT), building a structured knowledge graph -- not a narrative summary. The assessor gets a queryable evidence database, not a text paragraph. | 03_under_the_hood.md Section 6 ("The solution will not provide you a generated answer. It's not going to be like ChatGPT stuff, it's going to be a set of templated documents"); 02_mhra_workflow.md Section 7 (competitive baseline: ChatGPT for literature search) | Side-by-side comparison: give ChatGPT and the ArcaScience pipeline the same published observational study. Show the difference between a text summary and a structured extraction with full traceability. | Ready |

---

## Theme D: Confidentiality

MHRA handles highly sensitive patient-level and commercially confidential data. These constraints are non-negotiable and shape the entire collaboration model.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 28 | 40:48 -- Sharinto | "In terms of confidentiality... that's obviously where we'll be coming from in terms of protecting that information, because it would obviously be highly sensitive." | Confidentiality is foundational. MHRA handles patient-level and commercially confidential regulatory submissions. Any data sharing arrangement must have robust protections from day one. | We have designed a three-tier data classification model specifically for MHRA engagement. Tier 1 (public domain): PubMed, ClinicalTrials.gov, published regulatory assessments, safety communications -- processed on ArcaScience infrastructure, no MHRA data involved. Tier 2 (licensed databases): FAERS, EudraVigilance public data -- accessible via standard licensing. Tier 3 (MHRA internal): Yellow Card, CPRD, ongoing investigations -- never leaves MHRA infrastructure under any circumstances. The PoC operates entirely within Tier 1. Zero MHRA data enters ArcaScience systems. | 04_data_governance.md Section 2 (three-tier classification model); Section 3 Model A (public-domain PoC architecture diagram); Section 6 (PoC-first approach rationale) | Present the three-tier data classification model with clear diagram. Confirm in writing that the PoC involves zero MHRA data. | Ready |
| 31 | 42:42 -- Allison | "We might use CPRD... 30 million patient records over 30 years. We can't give you that." | CPRD (30M patients, 30 years of records) cannot be shared with ArcaScience. This is one of MHRA's primary real-world evidence sources and it is definitively off-limits for external access. | Understood. CPRD data will not be requested, accessed, or processed by ArcaScience on its own infrastructure at any stage. If future phases warrant CPRD integration, the only viable model is an on-premises deployment where ArcaScience models are deployed within MHRA's secure environment and CPRD data never leaves that boundary. This "bring the algorithm to the data" pattern is architecturally feasible (Kubernetes-based, containerised) but has not yet been engineered for MHRA's specific requirements. CPRD integration is a Phase 2+ discussion, contingent on PoC success and MHRA's interest. | 04_data_governance.md Section 4 (CPRD in-place query model with architecture diagram); Section 3 Model B (on-premises deployment architecture); 03_under_the_hood.md Section 7 (on-premises feasibility assessment) | Diagram showing "bring the algorithm to the data" model for CPRD. Honest statement that this is architecturally feasible but not yet built for MHRA. | TBD -- on-premises deployment for MHRA not yet engineered; CPRD connector not built |
| 32 | 43:13 -- Allison | "Can't give you our yellow card data because that's far too confidential. Because it's patient level data. We're not allowed to share that." | Yellow Card data is off-limits due to patient-level confidentiality. Along with CPRD, these are MHRA's two most important proprietary data sources. The PoC must work without both. | Confirmed: Yellow Card data will not be requested or accessed. The PoC uses only public-domain data. For published aggregate spontaneous reporting data (e.g., FAERS data, published Yellow Card statistics in MHRA Drug Safety Updates), the system can extract and structure these published summaries. But raw, patient-level Yellow Card data is classified as Tier 3 and subject to the same on-premises-only constraint as CPRD. | 04_data_governance.md Section 2 Tier 3 (Yellow Card classified as strictly confidential); Section 3 Model A (PoC uses no MHRA data) | PoC deliverable explicitly notes which evidence it could not access (Yellow Card, CPRD) as part of the gap analysis, demonstrating the system knows what it cannot see. | Ready |
| 39 | 52:07 -- Allison | "Most of the things that we do are actually highly confidential and I don't think we could share an ongoing safety issue with you." | Ongoing safety issues cannot be shared with an external vendor. Any PoC must use a COMPLETED, publicly disclosed safety issue. This is a hard constraint on collaboration scope. | Agreed. The PoC is designed exclusively around completed, publicly reported safety issues. The three proposed candidates (SGLT2/DKA, fluoroquinolones, valproate) are all historical cases with published regulatory conclusions from MHRA, EMA, and FDA. MHRA selects the case. ArcaScience works from public data only. MHRA compares outputs against their own historical assessment internally -- they do not need to share their assessment with ArcaScience. | 06_poc_plan.md Section 1 criterion #1 ("Closed or publicly reported"); 04_data_governance.md Section 6.2 (alignment with MHRA's own proposal) | Present the three PoC candidates and let MHRA choose. Confirm in the collaboration letter that no ongoing safety issues will be discussed. | Ready |
| 43 | 53:51 -- Allison | "We're working with multiple companies and multiple safety issues in different scenarios, and with very highly sensitive patient level data." | Confidentiality is structurally more complex than with pharma clients. MHRA simultaneously handles data from MULTIPLE companies and must ensure data from one company's product is never visible in work on another's. | This multi-tenant confidentiality requirement is acknowledged. ArcaScience's current platform supports role-based access control (RBAC) via Keycloak with JWT-based authentication. For any future deployment involving MHRA internal data, complete data isolation between different drug/company investigations would be required. The architecture supports project-level data segregation. However, the specific multi-tenant isolation requirements for a UK government regulator would need to be scoped with MHRA's IT security team. The PoC sidesteps this entirely by using only public-domain data with no company-specific confidential inputs. | 04_data_governance.md Section 5.1 (RBAC, Keycloak, JWT); Section 5.3 (additional UK government requirements -- Cyber Essentials Plus, DSPT, UK GDPR all TBD); 03_under_the_hood.md Section 7 (authentication architecture) | For the PoC: confirm zero MHRA data involvement. For future phases: present RBAC architecture and invite MHRA IT to assess whether it meets their multi-tenant requirements. | TBD -- multi-tenant isolation for UK government regulator requirements not yet assessed |

---

## Theme E: Data Sources and Quality

MHRA assessors work with messy, incomplete, heterogeneous data. The system must handle variable-quality inputs without producing skewed outputs, and must be transparent about what it cannot access.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 2 | 04:35 -- Steph | "How are you ensuring the quality of those data sources... when you're looking at data sources, how are you ensuring the quality of those data sources." | Steph wants to understand data quality assurance mechanisms. Not just what sources are used, but how the platform validates that input data is reliable, complete, and fit for purpose. | The system processes data from curated public sources (PubMed, ClinicalTrials.gov, FAERS, regulatory databases). It does not assess the scientific quality of an individual source -- that is the assessor's role. What the system does: it extracts study design elements (sample size, duration, blinding, confounders) that allow the assessor to make quality judgments efficiently. It also flags incompleteness -- if a study does not report confidence intervals or confounder adjustment, that absence is noted in the extraction output. The system surfaces what is present and what is absent; the assessor determines what that means for reliability. | 03_under_the_hood.md Section 6 (extraction vs. judgment table -- quality assessment excluded from system scope); 04_data_governance.md Section 2 Tier 1 (curated public sources listed) | Show extraction output for a study that has strong methodology vs. one with weak methodology. Demonstrate that the system extracts the elements that distinguish them, but does not label either as "high quality" or "low quality." | Ready |
| 8 | 18:30 -- Sharinto | "In terms of that quality control... the information is going to be variable. Some forms of information may not necessarily be complete." | The platform must handle incomplete, inconsistent, and variable-quality data without producing skewed outputs. This is the practical reality of post-authorisation data. | The system processes whatever data is available in the specified sources. When information is incomplete (e.g., a study does not report sample size, or a case report lacks patient demographics), the extraction output reflects those gaps rather than inferring missing data. Fields that cannot be populated from the source are marked as "not reported" rather than left blank or filled with assumptions. The gap analysis deliverable identifies systematic gaps across the evidence base (e.g., "no studies reporting UK-specific incidence identified"). | 03_under_the_hood.md Section 6 (extraction does not infer missing data); 06_poc_plan.md Section 4.6 (gap analysis deliverable specification) | Show extraction from an incomplete source. Demonstrate that "not reported" fields are explicitly marked, not silently omitted. Show the gap analysis flagging systematic data absences. | Ready |
| 18 | 31:45 -- Allison | "I don't believe that data is available easily or with any confidence for drugs on the market. So we're looking across 28 different antidepressants here." | Allison challenges the claim that effectiveness data is easily available for marketed drugs. For 28 antidepressants, real-world effectiveness data is NOT easily available or reliable. ArcaScience must not overstate data availability. | We acknowledge that real-world effectiveness data for long-marketed drugs is scarce, fragmented, and often of uncertain quality. The system works with whatever published evidence exists -- it does not claim comprehensive coverage. The gap analysis deliverable is specifically designed to make explicit what evidence could not be found. If the available literature for a class of 28 antidepressants is sparse on effectiveness endpoints, the system will report that gap rather than presenting incomplete data as sufficient. The assessor decides whether the available evidence is adequate for their assessment. | 02_mhra_workflow.md Section 4.3 (data quality is heterogeneous); 05_positioning_findings.md Section 3.2 (safer reframing of volume claims -- "Data volume is less important than data relevance and quality"); 06_poc_plan.md Section 4.6 (gap analysis) | Gap analysis in the PoC deliverable: clearly show which evidence types were NOT found, not just what was found. | Ready |
| 19 | 32:07 -- Allison | "We are dependent on real world databases, electronic health records or observational studies, which are not easy to derive benefits from." | MHRA's primary post-auth data sources are inherently difficult to work with. The platform must be designed for these messy data types, not just clean clinical trial data. | The system's extraction models are trained on diverse document types including observational studies, which have fundamentally different structures from RCTs (no randomisation, no blinding, different outcome definitions, different confounding structures). The models extract the study design elements that signal how the evidence should be interpreted: observational design, retrospective vs. prospective, confounder adjustment approach, exposure definition, follow-up duration. The system presents these elements; the assessor determines whether the study's design supports the conclusions drawn. The system does not derive benefits from real-world data -- it structures the published evidence so the assessor can perform that evaluation more efficiently. | 03_under_the_hood.md Section 2 (training data spans all document types and phases); Section 6 (extraction categories include study design elements) | Demo: process a published observational study and show extracted study design elements that differentiate it from an RCT extraction. Highlight the different extraction profile. | Ready |
| 29 | 41:03 -- Sharinto | "How are you managing healthcare information? Is that where your clients are coming to you with a particular source for that healthcare information and you integrate it into the platform." | Sharinto wants to understand the data flow for healthcare information -- who controls it, where it comes from, how it is integrated. This relates to data governance and the operational model for any MHRA engagement. | The data flow depends on the engagement model. For the PoC, ArcaScience uses only public-domain sources that it already indexes in its Profiling Base (PubMed, ClinicalTrials.gov, FAERS, published regulatory documents). No MHRA data is involved. For pharmaceutical clients, the client typically provides their data (clinical study reports, internal databases), which is ingested into the pipeline within the client's data environment. Data does not leave the client's infrastructure when confidentiality requires it. For MHRA specifically, any future phase involving internal data would follow the on-premises model where ArcaScience tools are deployed within MHRA's environment and data never crosses the organisational boundary. | 04_data_governance.md Section 3 Model A (PoC data flow) and Model B (on-premises data flow); 03_under_the_hood.md Section 7 (deployment options and data confidentiality architecture) | Data flow diagram showing the PoC model (public data only, no MHRA data) and the future on-premises model (MHRA data stays within MHRA). | Ready |
| 33 | 43:44 -- Allison | "Most of the data is not in a nice structured form... a lot of it will be literature. So how do you handle published studies?" | The primary available data for a PoC is published literature, which is unstructured text. Allison wants to know specifically how the platform handles published studies -- not just case reports but complex study designs. | Published literature is the primary data type the system was built to process. The pipeline ingests published articles (PDF, XML, HTML), classifies the document type (RCT, observational study, case report, meta-analysis, etc.), identifies structural sections (abstract, methodology, results, discussion), and runs task-specific extraction models on each section. From the methodology section: study design, population criteria, sample size, duration. From the results section: effect sizes, confidence intervals, primary and secondary outcomes. From the safety section: adverse events, temporality, severity. Each extraction is linked to the specific section and paragraph of the source document. | 03_under_the_hood.md Section 1 (full pipeline diagram with processing stages); Section 4 (step-by-step processing example); 06_poc_plan.md Section 4.2 (structured extraction specification) | Live demo: ingest a published observational study as a PDF, process it through the pipeline, show the structured extraction output with paragraph-level source links. | Ready |
| 36 | 47:22 -- Allison | "I'm not talking about case reports. I'm talking about big observational studies or meta-analyses... where you don't have patient level data, where you're trying to understand the quality of that record." | Allison's real need is for observational studies and meta-analyses -- fundamentally different data structures from case reports. The demo must show capability with aggregate-level, complex study designs, not individual case report extraction. | The document classification model distinguishes between case reports, observational studies, meta-analyses, RCTs, and systematic reviews. Each document type has a tailored extraction profile. For observational studies, the system extracts: cohort definition, exposure groups, follow-up period, adjusted and unadjusted estimates, confounders assessed, and limitations. For meta-analyses, it extracts: included study count, search strategy, pooled estimates, heterogeneity measures (I-squared where reported), and subgroup analyses. The quality assessment of each study remains the assessor's responsibility -- the system provides the structured elements that enable that assessment. | 03_under_the_hood.md Section 2 (document type classification includes observational studies and meta-analyses); 06_poc_plan.md Section 4.2 (extraction specification for different study types) | Demo must include at least one observational study and one meta-analysis, NOT just case reports. Show the extraction differences between document types. | TBD -- meta-analysis-specific extraction template specifics need confirmation |

---

## Theme F: UK Context

Drug usage patterns, prescribing practices, and safety profiles differ by country. MHRA needs tools that account for UK-specific context, not global averages.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 16 | 26:59 -- Allison | "Usage of drugs is very different across the world, and safety events do not necessarily translate across the world... we see different safety events depending on how a drug is used first line, second line, third line." | UK-specific context matters enormously. Drug usage patterns and safety profiles differ by country. The platform must account for UK-specific treatment pathways, not just global averages. | The system extracts geographic context from published studies (study population location, healthcare system context) when reported. It can filter and flag evidence by geography when studies report country-specific data. For UK-specific sources, the system can ingest MHRA Drug Safety Updates, NICE guidelines, BNF entries, and UK-based observational studies from the public domain. However, contextualising benefit-risk by line of therapy and available alternatives requires assessor expertise and knowledge of UK clinical practice that the system does not possess. The system surfaces the published data; the assessor applies their knowledge of UK-specific prescribing patterns and treatment pathways. | 02_mhra_workflow.md Section 3d (UK context matters); 04_data_governance.md Section 2 Tier 1 (includes MHRA public sources, UK-specific SmPCs from MHRA website) | In the PoC, include UK-specific sources (MHRA Drug Safety Updates, UK-authored studies) alongside international sources. Tag and flag which evidence is UK-derived and which is from other jurisdictions. | TBD -- UK-specific source tagging and geographic filtering not confirmed as a current pipeline feature |
| 17 | 27:25 -- Allison | "If there's a drug you use third line, and there's no other alternative for a patient, then the benefit risk is different to a first line therapy." | The benefit-risk calculus changes based on line of therapy and available alternatives. The platform must support contextualised analysis based on where a drug sits in the treatment pathway, which varies by country and indication. | The system can extract line-of-therapy information when it is reported in published studies and clinical guidelines (e.g., "recommended as second-line therapy after metformin failure"). It can also extract information about available alternatives from drug class analyses and treatment guidelines. However, synthesising this into a context-dependent benefit-risk judgment is the assessor's role -- they bring knowledge of UK formulary positioning, patient demographics, and clinical practice that cannot be derived from literature alone. The system provides the evidence elements (treatment positioning data, comparative safety profiles); the assessor weighs these in context. | 02_mhra_workflow.md Section 3e (context-dependent benefit-risk weighting); 03_under_the_hood.md Section 6 (system extracts, does not judge) | If line-of-therapy data is present in the PoC literature, show its extraction. Demonstrate that the system can surface NICE guideline positioning for the drug class under review. | TBD -- line-of-therapy extraction not explicitly validated as a pipeline feature |

---

## Theme G: Messaging

ArcaScience's marketing language actively alienated the senior MHRA decision-maker. All communications must be completely reframed for regulatory audiences.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 4 | 11:58 -- Allison | "My antibodies are going through the roof just because it says fill your benefit risk in seconds. Don't spend months looking at it. Which already raises loads and loads of concerns." | The marketing messaging ("benefit risk in seconds") is deeply counterproductive with a senior regulator. It sounds irresponsible and trivialising of a process MHRA takes very seriously. This single phrase damaged credibility more than any technical gap. | We acknowledge that this messaging was inappropriate for a regulatory audience and did not accurately reflect how the system works. The system does not produce a benefit-risk assessment in seconds. What happens rapidly is data retrieval and initial structuring -- the evidence consolidation phase. The interpretive work, quality assessment, and benefit-risk judgment proceed on the assessor's timeline and require the assessor's full expertise. All MHRA-facing materials will be revised: the phrase "in seconds" will not appear in any communication with MHRA. The safer framing is: "The evidence consolidation phase -- gathering, structuring, and cross-referencing published literature -- is reduced from weeks to hours, freeing assessor time for the interpretive and judgment work that only qualified experts can perform." | 05_positioning_findings.md Section 1.1 (speed claims rated HIGH risk); Section 2 prohibition #2 (never claim results "in seconds"); Section 3.1 (complete table of safer replacements for all speed claims); Appendix priority action #2 (IMMEDIATE removal) | All MHRA-facing materials revised per 05_positioning_findings.md. Opening slide reframed: "AI-supported evidence structuring for post-authorisation safety assessment." No speed claims in any material, including the beta UI. | Ready |
| 20 | 32:28 -- Allison | "Benefit is the most challenging thing we have to try and get a handle on across the different populations and different ages, comorbidities, ethnicities, whatever. And you make it sound so easy, and it's so so hard." | ArcaScience is perceived as trivialising the complexity of benefit assessment across heterogeneous populations. This is a tone and credibility issue that undermines trust in any technical claims. | We do not believe benefit assessment is easy. We recognise it as one of the most complex analytical challenges in regulatory science -- requiring deep understanding of population heterogeneity, co-morbidity interactions, treatment context, and real-world clinical practice across diverse demographics. The system does not perform benefit assessment. It structures the published evidence that assessors use as input to their benefit assessment. The difficulty of the assessment itself is unchanged; the system aims only to reduce the manual effort of gathering and organising the evidence base. In all future communications, we will lead with acknowledgment of the genuine difficulty of regulatory science and position the system as one component of a rigorous, human-led process. We will follow Principle 4 from our messaging guidelines: "Acknowledge limitations before stating capabilities." | 05_positioning_findings.md Principle 4 ("Acknowledge limitations before stating capabilities"); Principle 5 ("Never claim to replace or automate benefit-risk assessment"); Section 2 prohibition #15 (permanently ban trivialising language); Supplementary Principle 6 (respect MHRA's specific context) | Follow-up meeting opens with explicit acknowledgment of the complexity of post-authorisation benefit-risk assessment. Prepared "Limitations and Boundaries" one-pager leads with what the system cannot do before describing what it can do. | Ready |

---

## Theme H: Collaboration Terms

These concerns address the practical structure of the engagement: data sourcing burden, funding, PoC format, timeline, and next steps.

| # | Timestamp / Speaker | Concern / Question | Why It Matters to MHRA | Our Response | Evidence Pointer | Demo Artifact | Status |
|---|---|---|---|---|---|---|---|
| 27 | 40:08 -- Sharinto | "In terms of the data input, you're very much reliant on us to provide you with all of the sources that we would potentially need in order to collect the information and create those test cases." | Sharinto probes the operational burden: does MHRA have to do all the data sourcing and integration work? If so, the efficiency gain is significantly diminished. | For the PoC, ArcaScience performs all data sourcing independently. MHRA provides nothing except the selection of which safety issue to test. ArcaScience identifies, ingests, and processes all public-domain sources without MHRA input. The only MHRA resource required is approximately 3-4 hours of assessor time across the entire PoC (one 60-minute scoping meeting, one 90-minute review session, plus optional written feedback). For future phases involving MHRA's internal data, the data remains within MHRA systems and the operational model would be scoped jointly -- but data sourcing effort is minimised because the system connects to existing data stores rather than requiring manual data preparation. | 06_poc_plan.md Section 5 (timeline with MHRA resource commitment quantified at each phase); Section 4 (all deliverables produced by ArcaScience at own expense); 04_data_governance.md Section 3 Model A (zero MHRA data in PoC) | Present the PoC resource commitment table: ArcaScience does all the work; MHRA invests approximately 3-4 hours of assessor time over 4 weeks. | Ready |
| 30 | 41:27 -- Sharinto | "I'm just trying to get a handle of where the information is coming from and what we would need to consider if we were to want to move forward with a case." | Sharinto is thinking practically about prerequisites, data requirements, and operational considerations before MHRA could commit to even a PoC. | The prerequisites for the PoC are minimal: (1) MHRA selects a completed, public safety issue, (2) MHRA confirms the scope in writing (drug class, safety concern, time period), (3) MHRA designates 1-2 assessors for the review session. No data sharing, no IT integration, no procurement process, no budget allocation required. ArcaScience does everything else. After the PoC, MHRA decides whether to continue. If not, no resources were committed beyond assessor time and no data was shared -- zero downside. | 06_poc_plan.md Phase 0 (joint scoping with minimal prerequisites); Section 5 (full timeline with resource commitments quantified); 04_data_governance.md Section 6.4 (decision gate diagram -- clear off-ramp if PoC underperforms) | Present a one-page "What MHRA Needs to Provide" summary: (1) choice of safety issue, (2) written scope confirmation, (3) assessor availability for one review session. Nothing else. | Ready |
| 40 | 52:47 -- Allison | "Whether we could look at a safety issue that's already in the public domain that we've already reported on. And kind of set you the challenge as to what you would find." | Allison proposes the PoC format herself: a previously concluded, publicly reported safety issue used as a blind test. She wants to compare what the platform would have found versus what MHRA actually found. She is also signalling that she still does not understand the platform's output format or practical utility. | This is exactly the PoC format we propose. We have prepared three candidate safety issues that meet the selection criteria: (A) SGLT2 inhibitors and DKA (recommended -- medium-high complexity, low political sensitivity, rich multi-regulator documentation), (B) fluoroquinolone antibiotics and disabling side effects (backup -- very high complexity, recent UK-specific action in 2023-2024), (C) sodium valproate and pregnancy risks (reserve -- very high complexity but politically sensitive due to Cumberlege Review). MHRA selects one, or proposes an alternative. ArcaScience runs its pipeline and delivers: evidence map, structured extraction, templated draft sections, traceability report, confidence scoring, and gap analysis. MHRA compares against their own historical assessment internally. | 06_poc_plan.md Section 2 (three candidates with full rationale, source URLs, and complexity assessment); Section 2.1 (recommendation with reasoning); Section 4 (complete deliverables specification); Appendix A (side-by-side comparison table) | Present the three candidate options with the comparison table (06_poc_plan.md Appendix A). Let MHRA choose. Provide public source URLs for each option so MHRA can verify data availability before selecting. | Ready |
| 44 | 54:47 -- Allison | "You don't have funding for this. So it's not something that we could offer funding for at the moment." | No budget from MHRA for a PoC. ArcaScience must self-fund entirely. This is a hard financial constraint on the collaboration. | Accepted. ArcaScience will fund the PoC entirely at its own expense as a strategic investment in demonstrating value to a key regulatory authority. MHRA contributes assessor time for review and feedback only -- no financial commitment, no procurement process, no contractual obligation beyond a lightweight collaboration letter. The collaboration letter will explicitly confirm: no funding changes hands in either direction. | 06_poc_plan.md Design Principle #2 ("Zero funding required -- ArcaScience bears all cost"); Phase 0 (collaboration letter terms -- no funding, no IP implications) | Collaboration letter draft confirming zero-cost engagement for MHRA. Timeline showing all cost borne by ArcaScience. | Ready |
| 45 | 55:08 -- Allison | "It really would be a proof... some sort of proof of concept where we were just testing or challenging your system." | The next step is explicitly defined as a challenge test, not a purchase or partnership. MHRA will evaluate, not co-develop. ArcaScience must deliver a standalone demonstration that proves value independently. | Understood. The PoC is structured as a challenge test. ArcaScience delivers independently; MHRA evaluates. The success metrics include both quantitative measures (source completeness >= 90%, extraction accuracy >= 95%, traceability completeness 100%, error rate < 2%) and qualitative measures (assessor satisfaction, "beyond literature search" test, actionability assessment). MHRA has full discretion over whether to continue after the PoC. No commitment is implied by participating. | 06_poc_plan.md Section 3 (success metrics -- quantitative and qualitative with specific targets); Section 8 ("What this PoC IS and IS NOT") | Present success metrics for MHRA's agreement before the PoC begins, so evaluation criteria are transparent and pre-agreed. | Ready |
| 46 | 57:51 -- Allison | "Let us take it away, and think about it again... we'll have an internal conversation. Come back to you." | MHRA will deliberate internally before committing to next steps. ArcaScience should not push for an immediate meeting but should prepare materials that support MHRA's internal discussion. | We respect MHRA's deliberation process. We will not push for a meeting date. We will prepare and send materials that support MHRA's internal deliberation: (1) this question matrix showing we heard and addressed every concern raised, (2) the technical architecture document for "under the hood" review, (3) the PoC proposal with three candidate options and full source links, (4) the data governance framework with three-tier classification. All materials are in regulator-appropriate language following the messaging principles from 05_positioning_findings.md. When MHRA is ready to proceed, we are ready. | 06_poc_plan.md Phase 0 (MHRA confirms readiness on their timeline); 01_quotes.md Theme 8 analysis (internal deliberation required) | Package of materials for MHRA's internal review: question matrix, technical document, PoC proposal, data governance framework. All in measured, technical tone with no marketing language. | Ready |
| 47 | 58:06 -- Allison | "We'll try and think through working through the beta version. We've still got access to that right and then to see if we can see an actual good use case." | MHRA plans to independently explore the beta platform to identify a use case themselves. The beta must be compelling and navigable enough for MHRA staff to self-discover value without ArcaScience hand-holding. | We confirm MHRA retains beta access. We will ensure the beta environment is configured to support post-authorisation exploration, with example outputs from observational studies and post-marketing safety data types rather than clinical trial examples. We will remove all flagged marketing language ("in seconds," "100% regulatory acceptance rate," etc.) from the beta UI. We will provide a brief assessor navigation guide pointing MHRA staff to the features most relevant to their workflow: source traceability, extraction output review, evidence mapping, and gap analysis. The beta should speak for itself without requiring ArcaScience guidance. | 06_poc_plan.md Section 6 (preparation requirements before next meeting -- beta must be ready); 05_positioning_findings.md Appendix priority actions #1-2 (IMMEDIATE: remove "under review by MHRA" from deck; remove "in seconds" from all materials including beta) | Configure beta with post-auth content. Remove all flagged marketing language from the beta UI per 05_positioning_findings.md. Provide a one-page assessor navigation guide. | TBD -- beta configuration and marketing language removal need implementation before MHRA explores further |

---

## Status Summary

| Status | Count | Percentage |
|--------|-------|------------|
| **Ready** | 37 | 74% |
| **TBD** | 13 | 26% |
| **Total** | **50** | **100%** |

---

## TBD Items Requiring Action Before Next MHRA Contact

| Priority | # | Concern Summary | What Needs to Happen | Owner | Dependency |
|----------|---|----------------|---------------------|-------|------------|
| IMMEDIATE | 47 | Beta marketing language | Remove "in seconds," "100% regulatory acceptance rate," "currently under review by MHRA," and all other flagged claims from beta UI and website | ArcaScience Product + Marketing | 05_positioning_findings.md Appendix |
| IMMEDIATE | 50 | Beta post-auth content | Configure beta with post-authorisation example outputs (observational studies, safety communications) rather than pre-auth clinical trial examples | ArcaScience Product | None |
| HIGH | 7 | Error propagation analysis | Develop and document formal error propagation analysis across the full 24-model chain (how 5% error at Step 3 compounds through Steps 4-6) | ArcaScience Engineering | 03_under_the_hood.md Section 4 TBDs |
| HIGH | 9 | Confidence scoring | Advance planned OKR features (per-insight confidence scores, disagreement flags, missing evidence indicators) to at least a demonstrable prototype | ArcaScience Product + Engineering | OKR Initiative 3 |
| HIGH | 22 | Bradford Hill evidence extraction | Confirm and validate that pipeline extracts temporality, dose-response, consistency, specificity, biological plausibility markers as structured elements | ArcaScience Engineering | None |
| HIGH | 37 | Meta-analysis extraction | Confirm meta-analysis-specific extraction template (I-squared, forest plot data, pooled estimates, subgroup analyses) is operational | ArcaScience Engineering | None |
| HIGH | 36 | Meta-analysis extraction (duplicate) | Same as #37 -- confirm document-type-specific extraction profiles for meta-analyses and large observational studies | ArcaScience Engineering | None |
| MEDIUM | 16 | UK-specific source tagging | Implement and validate geographic tagging and UK-source filtering in the extraction pipeline | ArcaScience Engineering | None |
| MEDIUM | 17 | Line-of-therapy extraction | Validate extraction of line-of-therapy data from clinical guidelines (NICE, BNF) | ArcaScience Engineering | None |
| MEDIUM | 41 | Concurrent multi-issue operations | Scope operational model for 80 concurrent pipeline configurations on Kubernetes infrastructure | ArcaScience Engineering | Phase 2 |
| MEDIUM | 43 | Multi-tenant data isolation | Assess RBAC architecture against UK government security requirements (Cyber Essentials Plus, DSPT, UK GDPR) | ArcaScience Engineering + Legal | Phase 2; requires MHRA IT input |
| MEDIUM | 49 | Longitudinal evidence tracking | Confirm and prepare a demo showing temporal evidence accumulation for a single safety question over multiple years | ArcaScience Product | None |
| LOW | 31 | CPRD on-premises deployment | Not needed for PoC. Scope CPRD connector and air-gapped deployment for Phase 2+ only after PoC success | ArcaScience Engineering | Phase 2; requires CPRD governance approval |

---

## Cross-Reference: Scratch Files Used

| Scratch File | Content | Questions Addressed |
|---|---|---|
| `scratch/01_questions.json` | 50 structured questions from transcript (primary input) | All 50 rows |
| `scratch/01_quotes.md` | Thematic analysis with speaker summaries and priority matrix | Context for all themes; speaker characterisation; priority ranking |
| `scratch/02_mhra_workflow.md` | MHRA post-auth workflow, data sources, assessment process, pharma vs. regulator comparison | #1, 6, 10, 11, 15, 16, 17, 18, 19, 22, 24, 26, 41, 42 |
| `scratch/03_under_the_hood.md` | Technical architecture, model validation, auditability, error localisation, deployment options | #2, 3, 5, 6, 7, 9, 11, 13, 14, 19, 25, 29, 31, 33, 34, 35, 37, 38, 42, 43, 48 |
| `scratch/04_data_governance.md` | Confidentiality framework, three-tier data classification, on-prem model, PoC-first rationale | #1, 8, 12, 16, 27, 28, 29, 30, 31, 32, 39, 43 |
| `scratch/05_positioning_findings.md` | Claims audit, safer reframings, messaging principles, "Claims We Will NOT Make" list, visual diagram specifications | #4, 20, 22, 24, 38, 42, 47, 48 |
| `scratch/06_poc_plan.md` | PoC candidates with source URLs, selection criteria, timeline, deliverables, success metrics, risk register | #1, 8, 9, 12, 13, 21, 23, 24, 26, 27, 30, 33, 34, 36, 39, 40, 41, 44, 45, 46, 47 |

---

*This document was prepared by Agent 7 (Editor / Deck Strategist) for internal ArcaScience use in MHRA collaboration preparation. All 50 questions from 01_questions.json are addressed. All responses are sourced from documented materials in the scratch files. Items marked TBD reflect gaps in current documentation or engineering readiness -- no capabilities have been fabricated. Every response maintains the boundary between what the system does (extraction/structuring) and what the assessor does (judgment/decision-making).*
